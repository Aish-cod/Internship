{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "9d9e0714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\aisha\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\aisha\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "0b53f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcd845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\aisha\\Downloads\\chromedriver_win32 (1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907604af",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the website on automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245dbd8f",
   "metadata": {},
   "source": [
    "Write a python program which searches all the product under a particular product from www.amazon.in.\n",
    "The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search\n",
    "for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4885ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "userinput = input(\"'Enter the product that we want to search : ':\")\n",
    "print(\"Your search is: \" + userinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "designation.send_keys(userinput)\n",
    "\n",
    "#click on the search button\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1eff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f85791",
   "metadata": {},
   "source": [
    "# Q-2 In the above question, now scrape the following details of each product listed in first 3 pages of your\n",
    "search results and save it in a data frame and csv. In case if any product has less than 3 pages in search\n",
    "results then scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "0000b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "\n",
    "#Opening the website on automated chrome browser\n",
    "\n",
    "driver.get(\"https://www.amazon.in/s?k=guitar&ref=nb_sb_noss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching URLs to open the pages\n",
    "\n",
    "urls = []   \n",
    "\n",
    "# for loop to scrape 3 pages\n",
    "for i in range(0,3):      \n",
    "    page_url = driver.find_elements(By.XPATH,\"//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\")\n",
    "    for i in page_url:\n",
    "        urls.append(i.get_attribute(\"href\"))\n",
    "        next_button = driver.find_element(By.XPATH,\"//a [@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\")\n",
    "        time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a856e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name = []\n",
    "name_of_product = []\n",
    "price = []\n",
    "exchange = []\n",
    "exp_delivery = []\n",
    "availability = []\n",
    "other_details = []\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "\n",
    "    #scrapping brand name of the product from the given page            \n",
    "    try:\n",
    "        \n",
    "        brand = driver.find_element(By.XPATH,\"//a[@id='bylineInfo']\")\n",
    "        brand_name.append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        brand_name.append('-')\n",
    " \n",
    "   #scrapping name of the product from the given page\n",
    "    try:\n",
    "        \n",
    "        product_name=driver.find_element(By.XPATH,\"//span[@id='productTitle']\")\n",
    "        name_of_product.append(product_name)\n",
    "    except NoSuchElementException:\n",
    "        name_of_product.append('-')\n",
    "              \n",
    "   #scrapping price from the given page\n",
    "    try:\n",
    "        \n",
    "        product_price=driver.find_element(By.XPATH,\"//span[@class='a-price-whole']\")\n",
    "        price.append(product_price)\n",
    "    except NoSuchElementException:\n",
    "        price.append('-')\n",
    " \n",
    "   #scrapping return/exchange from the given page\n",
    "    try:\n",
    "        \n",
    "        exch=driver.find_element(By.XPATH,\"//span[@class='a-declarative']/div/a\")\n",
    "        exchange.append(exch.text)\n",
    "    except NoSuchElementException:\n",
    "        exchange.append('-')\n",
    "\n",
    "   #scrapping expected_delivery from the given page\n",
    "    try:\n",
    "        \n",
    "        product_delivery=driver.find_element(By.XPATH,\"//div[@class='a-section a-spacing-mini']/b\")\n",
    "        exp_delivery.append(product_delivery.text)\n",
    "    except NoSuchElementException:\n",
    "        exp_delivery.append('-')\n",
    "\n",
    "    #scrapping availability from the given page\n",
    "    try:\n",
    "            \n",
    "        product_available=driver.find_element(By.XPATH,\"//span[@class='a-size-medium a-color-success']\")\n",
    "        availability.append(product_available.text)\n",
    "    except NoSuchElementException:\n",
    "        availability.append('-')\n",
    " \n",
    "   #scrapping product_url from the given page\n",
    "    try:\n",
    "            \n",
    "        prod_url=driver.find_element(By.XPATH,\"//ul[@class='a-unordered-list a-vertical a-spacing-mini']\")\n",
    "        other_details.append(prod_url.text)\n",
    "    except NoSuchElementException:\n",
    "        other_details.append('-')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db357cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(brand_name),\n",
    "len(name_of_product),\n",
    "len(price),\n",
    "len(exchange),\n",
    "len(exp_delivery),\n",
    "len(availability),\n",
    "len(other_details))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DataFrame for the scraped data\n",
    "\n",
    "product_guitar = pd.DataFrame({})\n",
    "product_guitar['Brand Name'] = brand_name\n",
    "product_guitar['Name of the Product'] = name_of_product\n",
    "product_guitar['Price'] = price\n",
    "product_guitar['Return/Exchange'] = exchange\n",
    "product_guitar['Expected Delivery'] = exp_delivery\n",
    "product_guitar['Availability'] = availability\n",
    "product_guitar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9925645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data in csv\n",
    "product_guitar.to_csv(\"Guitar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a26976",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde270d8",
   "metadata": {},
   "source": [
    "## Q.3 Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\aisha\\Downloads\\chromedriver_win32 (1)\")\n",
    "\n",
    "\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e27a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the website on automated chrome browser\n",
    "\n",
    "#driver.get(\"http://images.google.com/\")\n",
    "url = \"http://images.google.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "urls = []\n",
    "data = []\n",
    "\n",
    "search_item = [\"Fruits\",\"Cars\",\"Machine Learning\",\"Guitar\",\"Cakes\"]\n",
    "\n",
    "for item in search_item:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    #search_item.append(item.text)\n",
    "\n",
    "\n",
    "# finding webelement for search_bar\n",
    "search_bar = driver.find_element(By.XPATH,'//div[@class=\"YacQv\"]')\n",
    "\n",
    "# sending keys to get the keyword for search bar\n",
    "search_bar.send_keys(str(item))\n",
    "    \n",
    "# clicking on search button\n",
    "search_button = driver.find_element(By.XPATH,\"//button[@class='Tg7LZd']\").click()\n",
    "                                 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scroling down the webpage to get some more images\n",
    "\n",
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    \n",
    "    images = driver.find_elements(By.XPATH,\"//img[@class='rg_i Q4LuWd']\")\n",
    "    \n",
    "img_urls = []\n",
    "img_data = []\n",
    "    \n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "    \n",
    "    for i in img_url[:100]:\n",
    "        urls.append(i)\n",
    "        \n",
    "for i in range(len(urls)):\n",
    "    if i > 10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\" .format(i,10))\n",
    "    response = requests.get(urls[i])\n",
    "    \n",
    "    file = open(r\"C:\\Users\\aisha\\OneDrive\\Desktop\"+str(i)+\".jpg\",\"wb\")\n",
    "    \n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaabf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89f0971d",
   "metadata": {},
   "source": [
    "# Q.4 Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be\n",
    "scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1523c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\aisha\\Downloads\\chromedriver_win32 (1)\")\n",
    "\n",
    "# getting the webpage url\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdbcb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing login button\n",
    "login_button = driver.find_element(By.XPATH,\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "\n",
    "# search for web element\n",
    "search_bar = driver.find_element(By.XPATH,(\"//input[@class='_3704LK']\"))\n",
    "\n",
    "# sending keys to search product\n",
    "search_bar.send_keys(\"pixel 4A\")\n",
    "\n",
    "# location the search button using xpath\n",
    "search_button = driver.find_element(By.XPATH,\"//button[@class='L0Z3Pu']\")\n",
    "\n",
    "# clicking on search button\n",
    "search_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d983386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting 1st page of URLs of smartphone\n",
    "\n",
    "page1_url = []\n",
    "\n",
    "urls = driver.find_elements(By.XPATH,\"//a[@class='_1fQZEK']\")\n",
    "for url in urls:\n",
    "    page1_url.append(url.get_attribute('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a5c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Smartphones = ({})\n",
    "Smartphones['Brand Name'] = []\n",
    "Smartphones['Smartphone name'] = []\n",
    "Smartphones['Colour'] = []\n",
    "Smartphones['RAM'] = []\n",
    "Smartphones['Storage(ROM)'] = []\n",
    "Smartphones['Primary Camera'] = []\n",
    "Smartphones['Secondary Camera'] = []\n",
    "Smartphones['Display Size'] = []\n",
    "Smartphones['Battery Capacity'] = []\n",
    "Smartphones['Price'] = []\n",
    "Smartphones['Product URL'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data from each url of page 1\n",
    "for url in page1_url:\n",
    "    driver.get(url)\n",
    "    Smartphones['Product URL'].append(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #clicking on read more button to get more information\n",
    "    try:\n",
    "        read_more = driver.find_element(By.XPATH,\"//button[@class='_2KpZ6l _1FH0tX']\")\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception occured while moving to next page\")\n",
    "    \n",
    "    #getting brand name of smartphone\n",
    "    try:\n",
    "        brand_tags = driver.find_element(By.XPATH,\"//span[@class='B_NuCI']\")\n",
    "        Smartphones['Brand Name'].append(brand_tags.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Brand Name'].append('-')\n",
    "        \n",
    "    #getting name of smartphones\n",
    "    try:\n",
    "        name_tags = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][1]/table/tbody/tr[3]/td[2]/ul/li\")\n",
    "        Smartphones['Smartphone name'].append(name_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Smartphone name'].append('-')\n",
    "        \n",
    "       #getting colour of smartphone\n",
    "    try:\n",
    "        color_tags = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][1]/table/tbody/tr[4]/td[2]/ul/li\")\n",
    "        Smartphones['Colour'].append(color_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Colour'].append('-')\n",
    "        \n",
    "    #getting RAM data of smartphone\n",
    "    try:\n",
    "        ram_tags = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][4]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['RAM'].append(ram_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['RAM'].append('-')\n",
    "        \n",
    "    #getting ROM data of smartphones\n",
    "    try:\n",
    "        rom = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][4]/table[1]/tbody/tr[1]/td[2]/ul/li\")\n",
    "        Smartphones['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Storage(ROM)'].append('-')\n",
    "        \n",
    "    #getting  Primary camera data of smartphone\n",
    "    try:\n",
    "        pri =driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Smartphones['Primary Camera'].append(pri.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Primary Camera'].append('-')  \n",
    "             \n",
    "    #getting secondary camera data of smartphone\n",
    "    try:\n",
    "        sec = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[6]/td[1]\")\n",
    "        if sec != 'Secondary Camera' :\n",
    "            if driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[5]/td[1]\").text == \"Secondary Camera\":\n",
    "                sec_cam =driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[5]/td[2]/ul/li\")\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][5]/table[1]/tbody/tr[6]/td[2]/ul/li\")\n",
    "        Smartphones['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Secondary Camera'].append('-')\n",
    "         \n",
    "    #getting display size data of smartphone\n",
    "    try:\n",
    "        disp = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][2]/div\")\n",
    "        if disp.text != 'Display Features' : raise NoSuchElementException\n",
    "        disp_size = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][2]/table[1]/tbody/tr[1]/td[2]/ul/li\")\n",
    "        Smartphones['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Size'].append('-')\n",
    "           \n",
    "            \n",
    "        #getting the battery capacity of smartphone\n",
    "    try:\n",
    "        if driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][10]/div\").text != \"Battery & Power Features\" :\n",
    "            if driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][9]/div\").text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][9]/table/tbody/tr/td[1]\")\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][9]/table/tbody/tr/td[2]/ul/li\")\n",
    "            elif driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][8]/div\").text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][8]/table/tbody/tr/td[1]\")\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][8]/table/tbody/tr/td[2]/ul/li\")\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_tags = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][10]/table/tbody/tr/td[1]\")\n",
    "            if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_capa = driver.find_element(By.XPATH,\"//div[@class='_3k-BhJ'][10]/table/tbody/tr/td[2]/ul/li\")\n",
    "        Smartphones['Battery Capacity'].append(bat_capa.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Battery Capacity'].append('-')\n",
    "      \n",
    "    #getting price of smartphone\n",
    "    try:\n",
    "        price_tags = driver.find_element(By.XPATH,\"//div[@class='_30jeq3 _16Jk6d']\")\n",
    "        Smartphones['Price'].append(price_tags.text)\n",
    "    except NoSuchElementException:\n",
    "          Smartphones['Price'].append('-')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de128ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking lengths of all scraped data\n",
    "\n",
    "print(len(Smartphones['Brand Name']),\n",
    "      len(Smartphones['Smartphone name']), \n",
    "      len(Smartphones['Colour']),\n",
    "      len(Smartphones['RAM']),\n",
    "      len(Smartphones['Storage(ROM)']),\n",
    "      len(Smartphones['Primary Camera']),\n",
    "      len(Smartphones['Secondary Camera']),\n",
    "      len(Smartphones['Display Size']),\n",
    "      len(Smartphones['Battery Capacity']),\n",
    "      len(Smartphones['Price']),\n",
    "      len(Smartphones['Product URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framing the DataFrame\n",
    "\n",
    "df = pd.DataFrame.from_dict(Smartphones)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99bf4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data in csv\n",
    "df.to_csv(\"smartphones.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c8b18",
   "metadata": {},
   "source": [
    "# Q.5 Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\aisha\\Downloads\\chromedriver_win32 (1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d629d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting mentioned url and opening google maps web page\n",
    "url = 'https://www.google.co.in/maps'\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the city name in search bar\n",
    "\n",
    "City_name = input('Enter City name that has to be searched : ')\n",
    "search_bar = driver.find_element(By.ID,'searchboxinput')\n",
    "search_bar.click()\n",
    "time.sleep(2)\n",
    "\n",
    "#sending keys to find cities\n",
    "search_bar.send_keys(City_name)\n",
    "\n",
    "#checking for webelement and clicking on search button\n",
    "search_button = driver.find_element(By.ID,\"searchbox-searchbutton\")\n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    latitude_longitude = re.findall(r'@(.*)data',url_string)\n",
    "    \n",
    "    if len(latitude_longitude):\n",
    "        list_x = latitude_longitude[0].split(\",\")\n",
    "        if len(list_x)>=2:\n",
    "            latitude = list_x[0]\n",
    "            longitude = list_x[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(latitude, longitude))\n",
    "except Exception:\n",
    "        print(\"Error: \", str(Exception))   \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed97760",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397de95",
   "metadata": {},
   "source": [
    "# Q.6- Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818ad21",
   "metadata": {},
   "source": [
    "Skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bcd2a",
   "metadata": {},
   "source": [
    "# Q. 7 Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\aisha\\Downloads\\chromedriver_win32 (1)\")\n",
    "\n",
    "# opening the webpage\n",
    "\n",
    "url = \"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dea98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for best Laptop deal\n",
    "\n",
    "best_laptops = driver.find_element(By.XPATH,\"//div[@class='listing_container']//ul//li[9]\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "\n",
    "Laptop_Name = []\n",
    "Operating_sys = []\n",
    "Display = []\n",
    "Processor = []\n",
    "Memory = []\n",
    "Weight = []\n",
    "Dimensions = []\n",
    "Graph_proc = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6afcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the data of laptop names\n",
    "laptop_name = driver.find_elements(By.XPATH,\"//span[@class='datahreflink']\")\n",
    "for name in laptop_name:\n",
    "    Laptop_Name.append(name.text)\n",
    "\n",
    "    \n",
    "#getting the data of operating system\n",
    "try:\n",
    "    op_sys = driver.find_elements(By.XPATH,\"//div[@class='Specs-Wrap']\"\")\n",
    "    for os in op_sys:\n",
    "        Operating_sys.append(os.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#getting data of display of the Laptop\n",
    "try:\n",
    "    display = driver.find_elements(By.XPATH,\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "    for disp in display:\n",
    "        Display.append(disp.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of processor\n",
    "try:\n",
    "    processor = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[5]/td[3]\")\n",
    "    for pro in processor:\n",
    "        Processor.append(pro.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# scraping the data of memory\n",
    "try:\n",
    "    memory = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "    for memo in memory:\n",
    "        Memory.append(memo.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of weight\n",
    "try:\n",
    "    weight = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")\n",
    "    for wgt in weight:\n",
    "        Weight.append(wgt.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of dimensions\n",
    "try:\n",
    "    dimension = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\")\n",
    "    for dim in dimension:\n",
    "        Dimensions.append(dim.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping data of graph processor\n",
    "try:\n",
    "    graph = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[9]/td[3]\")\n",
    "    for gra in graph:\n",
    "        Graph_proc.append(gra.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "# scraping the data of price\n",
    "try:\n",
    "    price = driver.find_elements(By.XPATH,\"//td[@class='smprice']\")\n",
    "    for pri in price:\n",
    "        Price.append(pri.text.replace('₹ ','Rs'))\n",
    "except NoSuchElementException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d878e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Laptop_Name),\n",
    "len(Operating_system),\n",
    "len(Display),\n",
    "len(Processor),\n",
    "len(Memory),\n",
    "len(Weight),\n",
    "len(Dimensions),\n",
    "len(Graph_proc),\n",
    "len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a901e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating DataFrame for scraped data\n",
    "\n",
    "Gaming_Laptop=pd.DataFrame({})\n",
    "\n",
    "Gaming_Laptop['Laptop Name'] = Laptop_Name[0:7]\n",
    "Gaming_Laptop['Operating System'] =Operating_sys\n",
    "Gaming_Laptop['Display'] = Display\n",
    "Gaming_Laptop['Processor'] = Processor\n",
    "Gaming_Laptop['Memory'] = Memory\n",
    "Gaming_Laptop['Weight'] = Weight\n",
    "Gaming_Laptop['Dimensions'] = Dimensions\n",
    "#Gaming_Laptop['Graphical Processor'] = Graph_proc\n",
    "Gaming_Laptop['Price'] = Price\n",
    "Gaming_Laptop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data to csv\n",
    "Gaming_Laptop.to_csv(\"Gaming_Laptops.csv\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccca83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4903972",
   "metadata": {},
   "source": [
    "# Q.8  Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be\n",
    "scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\aisha\\Downloads\\chromedriver_win32 (1)\")\n",
    "\n",
    "# getting the specified url\n",
    "url = \"https://www.forbes.com/?sh=3b8d2bc62254\"\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eabdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get option button from the page\n",
    "option_btn = driver.find_element(By.XPATH,\"//div[@class='header__left']\")\n",
    "option_btn.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#select billionaires from options\n",
    "blns = driver.find_element(By.XPATH,\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "blns.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#select world billionaire\n",
    "bln_list = driver.find_element(By.XPATH,\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "bln_list.click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eec685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# empty lists of each column\n",
    "\n",
    "Rank = []\n",
    "Person_Name = []\n",
    "Net_worth = []\n",
    "Age = []\n",
    "Citizenship = []\n",
    "Source = []\n",
    "Industry = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1382ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    \n",
    "    # scraping the data of rank of the billionaires\n",
    "    rank_tag = driver.find_elements(By.XPATH,\"//div[@class='rank']\")\n",
    "    for rank in rank_tag:\n",
    "        Rank.append(rank.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    " \n",
    "    # scraping the data  of names of the billionaires\n",
    "    name_tag = driver.find_elements(By.XPATH,\"//div[@class='personName']/div\")\n",
    "    for name in name_tag:\n",
    "        Person_Name.append(name.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of age of the billionaires\n",
    "    age_tag = driver.find_elements(By.XPATH,\"//div[@class='age']/div\")\n",
    "    for age in age_tag:\n",
    "        Age.append(age.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of citizenship of the billionaires\n",
    "    cit_tag = driver.find_elements(By.XPATH,\"//div[@class='countryOfCitizenship']\")\n",
    "    for cit in cit_tag:\n",
    "        Citizenship.append(cit.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of source of income of the billionaires\n",
    "    sour_tag = driver.find_elements(By.XPATH,\"//div[@class='source']\")\n",
    "    for sour in sour_tag:\n",
    "        Source.append(sour.text)\n",
    "    time.sleep(1)\n",
    "     \n",
    "    # scraping data of industry of the billionaires\n",
    "    ind_tag = driver.find_elements(By.XPATH,\"//div[@class='category']//div\")\n",
    "    for ind in ind_tag:\n",
    "        Industry.append(ind.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping data of net_worth of billionaires\n",
    "    net_tag = driver.find_elements(By.XPATH,\"//div[@class='netWorth']/div\")\n",
    "    for net in net_tag:\n",
    "        Net_worth.append(net.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # clicking on next button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19155ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Rank),\n",
    "len(Person_Name),\n",
    "len(Net_worth),\n",
    "len(Age),\n",
    "len(Citizenship),\n",
    "len(Source),\n",
    "len(Industry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framing Data\n",
    "\n",
    "Billionaires = pd.DataFrame({})\n",
    "Billionaires['Rank'] = Rank\n",
    "Billionaires['Name'] = Person_Name\n",
    "Billionaires['Net Worth'] = Net_worth\n",
    "Billionaires['Age'] = Age\n",
    "Billionaires['Citizenship'] = Citizenship\n",
    "Billionaires['Source'] = Source\n",
    "Billionaires['Industry'] = Industry\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1289e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataset in csv\n",
    "Billionaires.to_csv('Forbes_Billionaires.csv')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523df2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "863efdc8",
   "metadata": {},
   "source": [
    "# Q. 9 Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\aisha\\Downloads\\chromedriver_win32 (1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the youtube.com\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6325e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for search bar\n",
    "search_bar = driver.find_element(By.XPATH,\"//div[@class='ytd-searchbox-spt']/input\")\n",
    "\n",
    "#Game of Throne youtube video\n",
    "search_bar.send_keys(\"GOT\")     \n",
    "time.sleep(2)\n",
    "\n",
    "#clicking on search button\n",
    "search_button = driver.find_element(By.ID,\"search-icon-legacy\")\n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# clicking on first video\n",
    "video = driver.find_element(By.XPATH,\"//yt-formatted-string[@class='style-scope ytd-video-renderer']\")\n",
    "video.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6205e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 times we scroll down by 10000 in order to generate more comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f034d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "comments_on_page = []\n",
    "comment_by_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []\n",
    "\n",
    "# get comments\n",
    "com_page = driver.find_elements(By.ID,\"content-text\")\n",
    "for i in com_page:\n",
    "    if i.text is None:\n",
    "        comments_on_page.append(\"--\")\n",
    "    else:\n",
    "        comments_on_page.append(i.text)\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "# get time when comment was posted\n",
    "\n",
    "t = driver.find_elements(By.XPATH,\"//a[contains(text(),'ago')]\")\n",
    "for i in t:\n",
    "    Time.append(i.text)\n",
    "    \n",
    "for i in range(0,len(Time),2):\n",
    "    comment_by_time.append(Time[i])\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "# scrape the comment likes\n",
    "like = driver.find_elements(By.XPATH,\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(comments_on_page),len(comment_by_time),len(No_of_Likes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for scraped data\n",
    "\n",
    "youtube = pd.DataFrame({})\n",
    "youtube['Comment'] = comments_on_page[:500]\n",
    "youtube['Comment Time'] = comment_by_time[:500]\n",
    "youtube['Comment Upvotes'] = No_of_Likes[:500]\n",
    "youtube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540852bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dataframe to csv\n",
    "youtube.to_csv(\"Youtube GOT game video.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b60043",
   "metadata": {},
   "source": [
    "# Q.10 Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews,\n",
    "overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb05d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the webdriver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\aisha\\Downloads\\chromedriver_win32 (1)\")\n",
    "\n",
    "# getting the web page of mentioned url\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65770028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locating the location search bar\n",
    "search_bar = driver.find_element(By.ID,'location-text-input-field')\n",
    "\n",
    "# entering London in search bar\n",
    "search_bar.send_keys(\"London\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2476e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select London\n",
    "London = driver.find_element(By.XPATH,\"//ul[@id='predicted-search-results']//li[2]\")\n",
    "\n",
    "#clicking on button\n",
    "London.click()\n",
    "\n",
    "# click on Go button\n",
    "search_button = driver.find_element(By.ID,'search-button')\n",
    "search_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04174aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list & find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "facilities = []\n",
    "description = []\n",
    "url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba84897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "483d9005",
   "metadata": {},
   "source": [
    "hostel name, distance from city centre, ratings, total reviews,\n",
    "overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list & find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "facilities = []\n",
    "description = []\n",
    "url = []\n",
    "      \n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(3)   \n",
    "    \n",
    "    # getting  hostel name\n",
    "    try:\n",
    "        name = driver.find_elements(By.XPATH,\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "            \n",
    "    # getting distance from city centre\n",
    "    try:\n",
    "        dist = driver.find_elements(By.XPATH,\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in name:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "    \n",
    "    #getting ratings\n",
    "    try:\n",
    "        rat = driver.find_element(By.XPATH,\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "        \n",
    "         # fetching over all review\n",
    "    try:\n",
    "        overall = driver.find_element(By.XPATH,\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    \n",
    "    \n",
    "    for i in driver.find_elements(By.XPATH,\"//div[@class='prices-col']\"):   \n",
    "        # getting privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element(By.XPATH,\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "   \n",
    "    for i in driver.find_elements(By.XPATH,\"//div[@class='prices-col']\"):          \n",
    "    \n",
    "    #getting dorms from price\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]/div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            \n",
    "        #getting total review\n",
    "    try:\n",
    "        rws = driver.find_element(By.XPATH,\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "              \n",
    "            \n",
    "    #getting facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements(By.XPATH,\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements(By.XPATH,\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text)\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "    \n",
    "          \n",
    "# fetching property description\n",
    "    try:\n",
    "        disc = driver.find_element(By.XPATH,\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    \n",
    "    \n",
    "     #fetching url of each hostel\n",
    "    p_url = driver.find_elements(By.XPATH,\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        url.append(i.get_attribute(\"href\"))\n",
    "        \n",
    "for i in url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc000d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hostel_name),\n",
    "len(distance),\n",
    "len(pvt_prices),\n",
    "len(dorms_price),\n",
    "len(rating),\n",
    "len(reviews),\n",
    "len(over_all),\n",
    "len(facilities),\n",
    "len(description),\n",
    "len(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating DataFrame\n",
    "\n",
    "Hostel = pd.DataFrame({})\n",
    "Hostel['Hostel Name'] = hostel_name\n",
    "Hostel['Distance from City Centre'] = distance\n",
    "Hostel['Ratings'] = rating\n",
    "Hostel['Total Reviews'] = reviews\n",
    "Hostel['Overall Reviews'] = over_all\n",
    "Hostel['Privates from Price'] = pvt_prices\n",
    "Hostel['Dorms from Price'] = dorms_price\n",
    "Hostel['Facilities'] = facilities[:74]\n",
    "Hostel['Description'] = description\n",
    "Hostel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ba1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hostel.to_csv(\"Hostels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0161903",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
